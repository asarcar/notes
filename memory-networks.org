* Progression of papers
- [[~/data-science/nlp/qa_papers/facebook/open-qa-with-weakly-embedded-models-04-2014.pdf][open QA with weakly embedded models]]
  - *Objective:* Open QA on an auto-generated KB known as the ReVerb KB
    - The main idea is to score a question against all the triples in the KB
      and return the best scoring triple
  - *Vector Embedding:* Introduces the idea of using vector embeddings for
    words to do QA
  - *BOW vector*: Uses the idea of BOW vector embedding for a sentence, i.e. a
    sentence is considered as a BOW (with word order not mattering). So the
    vector embeddings for all words in a sentence are added up to form a vector
    for the sentence
  - *Scoring:*
    $s(q,t) =  f(q)^T g(t)$

    Where $f(q) = V \Phi(q)$ and $g(the) = W \Psi(t)$

    Where V & W are embedding matrices, while $\Phi(q)$ and $\Psi(t)$ are BOW
    vector encoding presence/absence of dictionary words in a question $q$ and
    triple $t$
  - *Fine Tuning:* a matrix $M \in R^{k \times k}$ parameterizing the
     similarity between words and triples embeddings is used:
     $$s_{ft}(q,t) = f(q)^T M g(t)$$
  - *String Matching:* To avoid scoring all triples (consequently reducing
    noisy matches), only triples which have at least one word matching the
    question are considered
  - *Dataset:*
    - ReVerb KB
    - Question Paraphrases from WikiAnswers
    - Auto-generated QA from ReVerb KB
    - WebQuestions

- [[~/data-science/nlp/qa_papers/facebook/QA-with-subgraph-embeddings.pdf][QA with subgraph embeddings]]

  - *Key Idea:* Sum vector embeddings across all entities/relations in a
    KB subgraph emanating from entities referenced in the question

  - *Dataset:* OMDB (open-movie dataset)

- [[~/data-science/nlp/qa_papers/facebook/memory-networks-04-2014.pdf][Memory Networks]]

  - *Dataset:*
    - A subset of BabI (for Simulated World QA task)
    - ReVerb + WikiAnswers Dataset (for large scale simple QA task)
      - ReVerb KB 
      - Question Paraphrases from WikiAnswers
      - Auto-generated QA from ReVerb KB

- [[~/data-science/nlp/qa_papers/facebook/ai-complete-qa-set-of-toy-tasks.pdf][Towards AI-Complete QA]]

  - *Dataset:* BabI (introduced by this paper)

- [[~/data-science/nlp/qa_papers/facebook/end-to-end-memory-network-03-2015s.pdf][end-to-end memory networks]]

  - *Key Idea:* Memory networks required separate training for each hop because
    they were using a hard max when comparing Q to memories.  This also
    required strong supervision, i.e. a supporting sentence for each
    intermediate hop. To get around this, end-to-end memory networks were
    invented. They replace they hard max with a soft max allowing
    back-propagation across multiple hops.
  
  - *Dataset:* BabI
  
- [[~/data-science/nlp/qa_papers/facebook/large-scale-simple-quesion-answering-with-memory-networks.pdf][large scale simple QA]]

  - *Datasets:*
    - Simple Questions (Introduced by this paper)
    - WebQuestions
    - Auto-generated Questions from Freebase
    - Question Paraphrases from WikiAnswers
    - Gold set of QA on Reverb Set (introduced by Paralex paper)
   
  - *Transfer Learning:* learning done on Simple Questions and Auto-generated
    Freebase Questions can be applied to ReVerb without training

  - *Removing mediator nodes:* in freebase reduces path length between entities
    from 2 to 1, widely increasing the scope of simple QA task on Freebase.

  - *Grouping objects* of (subject, relation, object) relations with common
    subject and relation values

  - *Feature Representation of facts:*

    - Freebase Facts: using Bag-of-symbol vector where symbols are entities or
      relationships in Freebase

    - Questions: using Bag-of-ngrams vector (the dictionary consists of
      individual words appearing in Questions as well as aliases of Freebase
      Entities, each alias being a single n-gram)

    - Reverb Facts:

      - relations encoded using bag-of-words

      - entities encoded using either Freebase symbol or Freebase alias or bag
        of words.

  - *Candidate Generation:* to avoid matching all facts in the KB with the
    question, a set of candidate facts are generated which are then used for
    matching. The candidate sets are generated by generating all possible
    n-grams of the question (which do not contain an interrogative noun or a
    stop word). Only n-grams which are an alias of a Freebase entity are kept.
    All n-grams which are sub-sequence of an n-gram which is kept are
    discarded. For 5 longest matching n-grams, 2 entities with most links in
    Freebase are kept. All facts with these entities are candidates for
    matching.

- [[~/data-science/nlp/qa_papers/facebook/key-value-memory-networks-for-reading-documents.pdf][key-value-memory-networks]]

* QA Tasks
* Memory Networks
- *Matching Function:* If $x$ is the input and $y$ is a memory, then
  we score the match between the memory and input as 
  $$ s(x, y)  = \Phi_x(x)^T U^T U \Phi_y(y)$$
  Where $U$ is a $n \times D$ embedding matrix where $n$ is the embedding dimension and
  $D$ is the size of the vocabulary for $x$

* Memory Network Extensions
- *Efficient memory via hashing*
  - See 3.2 of [[~/data-science/nlp/qa_papers/facebook/memory-networks-04-2014.pdf][Memory Networks]]
- *Modelling implicit notion of time encoded in the order of sentences in a story:*
  - *Modelling write time* See 3.4 of [[~/data-science/nlp/qa_papers/facebook/memory-networks-04-2014.pdf][Memory Networks]]
  - *Temporal Enconding*: See 4.1 of [[~/data-science/nlp/qa_papers/facebook/end-to-end-memory-network-03-2015s.pdf][end-to-end memory networks]]
- *Modelling unseen words:* predict a word using its neighbouring (context)
  words, i.e. for each unseen word, store a bag of words that word has
  co-occurred with. Use a seperate set of embeddings for the context words
  which is learned at training time by probabilitically pretending that a word
  has not been seen before. See 3.5 of [[~/data-science/nlp/qa_papers/facebook/memory-networks-04-2014.pdf][Memory Networks]]
- *Exact Matches and Unseen Words:*
  - We would like to improve the matching score if input and memory had some
    exact word matches.
  - See 3.6 of [[~/data-science/nlp/qa_papers/facebook/memory-networks-04-2014.pdf][Memory Networks]]
- *Predicting multiple words:* use RNN or add a special word $w_{\emptyset }$ to the dictionary and predict word $w_i$ on each iteration $i$ conditional on previous word
  i.e. 
  $$w_i = argmax_{w \in W} \text{ } s_R([x, m_{o_1}, ..., m_{|o|}, w_1, ..., w_{i-1}], w_k)$$
  - for handling multi-word outputs
  - See end of 3.1 in [[~/data-science/nlp/qa_papers/facebook/memory-networks-04-2014.pdf][Memory Networks]]
  - See A.2 of [[~/data-science/nlp/qa_papers/facebook/ai-complete-qa-set-of-toy-tasks.pdf][Towards AI-Complete QA]]
- *Adaptive Memory:* for handling questions requiring chaining of multiple facts
- *Handling Word Order in a Sentence:*
  - *N-grams BOW:*  (instead of 1-gram BOW)
    - For handling cases where word ordering matters e.g. subject vs object
    - See A.2 of [[~/data-science/nlp/qa_papers/facebook/ai-complete-qa-set-of-toy-tasks.pdf][Towards AI-Complete QA]] 
  - *Positional Encoding:* See 4.1 of [[~/data-science/nlp/qa_papers/facebook/end-to-end-memory-network-03-2015s.pdf][end-to-end memory networks]]
- *Non-Linear matching function:* using a matching fn which is a classical 2
  layer neural-net with tanh neuron.
  $$ s(x, y)  = E(x)^T E(y) $$
  Where, $$ E(x)  = tanh(W tanh(U \Phi_x(x))$$
  Here $W$ is $n \times n$ matrix 
  - important for modeling 3-way interactions, e.g for yes/no questions, negations and indefinite knowledge
  - See A.2 of [[~/data-science/nlp/qa_papers/facebook/ai-complete-qa-set-of-toy-tasks.pdf][Towards AI-Complete QA]]

* Datasets 

|-------------------+--------------------------------+--------+-------------------------+----------|
| Dataset Name      | Description                    | Size   | Generation              | Curated/ |
|                   |                                |        | method                  | Noisy    |
|-------------------+--------------------------------+--------+-------------------------+----------|
|-------------------+--------------------------------+--------+-------------------------+----------|
| Reverb KB         | (subject, relation, object)    |        | Mined using Reverb from | Noisy    |
|                   | triples mined from ClueWeb09.  | 14m    | from ClueWeb09          |          |
|                   | 2M entities and 600K relations |        |                         |          |
|-------------------+--------------------------------+--------+-------------------------+----------|
| Paraphrase Corpus | (q, q') pairs                  | 35m    | Human annotated on      | Noisy    |
| (WikiAnswers)     |                                |        | WikiAnswers             |          |
|-------------------+--------------------------------+--------+-------------------------+----------|
| Gold set of       | (question, answer, label)      | 48k    | Human anotated using    | Curated  |
| QA pairs          | triples                        |        | question pairs from     |          |
| introduced by     |                                |        | paraphrase corpus which |          |
| Paralex Paper     |                                |        | are answerable using    |          |
|                   |                                |        | Reverb KB               |          |
|-------------------+--------------------------------+--------+-------------------------+----------|
| Facebook BabI     | 20 tasks with 1K/10K Qs        | 20K/   | Simulator               | Clean    |
| Dataset           |                                | 200K   |                         |          |
|-------------------+--------------------------------+--------+-------------------------+----------|
| SimpleQuestions   | (q,a) pairs, where q is a      | 100K   | Human annotators        | Clean    |
|                   | written in natural language    |        |                         |          |
|                   | by human annotators using a    |        |                         |          |
|                   | fact from Freebase. This fact  |        |                         |          |
|                   | also provides the answer       |        |                         |          |
|-------------------+--------------------------------+--------+-------------------------+----------|
| WebQuestions      | (q,a) pairs where 'a' is       | 5.8K   | Human anotator          | Clean    |
|                   | a list of aliases for Freebase |        |                         |          |
|                   | entities. Many questions in    |        |                         |          |
|                   | this dataset expect multiple   |        |                         |          |
|                   | answers                        |        |                         |          |
|-------------------+--------------------------------+--------+-------------------------+----------|
| Automatically     | Generated using 16 seed Q      | 250M   | Auto-generated          | Clean    |
| generated Qs      | templates applied to ReVerb    |        |                         |          |
| from ReVerb       | facts (with some constraints)  |        |                         |          |
|-------------------+--------------------------------+--------+-------------------------+----------|
| Automatically     | Generated using 16 seed Q      | 10/12M | Auto-generated          | Clean    |
| generated Qs      | templates applied to Freebase  |        |                         |          |
| from Freebase     | facts (with some constraints)  |        |                         |          |
|-------------------+--------------------------------+--------+-------------------------+----------|
|                   |                                |        |                         |          |
|                   |                                |        |                         |          |
