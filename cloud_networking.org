* SD-WAN
** Notes
*** Site: Identity 
*** GW: 
Identity: Correct IP address chosen: Internet/MPLS
Security: DoS, Internet ACL, QoS, etc.
Chicken & Egg: Do not rely on Controller to establish contact. 
Do not expect modify that may break the contact.
*** Peer with Cloud on a loopback address.

** Use Case: 
*** DB/BW Scale at different points of presence
*** Federated Model: Multi-Hop BGP with Source Route
** Design Objectives
*** App-aware large scale routing
**** Networking
***** SLOs: 
Maximize Goodput & Minimize Quality Issues (Latency, Jitter, and Reorder)
***** Allocate App-Level Priority Based Resources 
BW & Behavior based on app level priority 
***** Override Default Forwarding
Override BGP specified forwarding behavior at fine granularity. Legacy IP 
routing operates on low-level information i.e. BGP announcements & policies.
***** Operating app-aware fine-grained policies 
Fine policies require complex BGP rules that are hard to manage, 
reason, and implement.
**** Security
*** Safety: Global optimization is a global failure domain
Sanity Check, Canary Updates, Fail-Static, and Revert to last known 
good state. Provide BRB buttons at various levels.
*** Incremental Brownfield Deployment: No forklift upgrade
*** High Availability & Feature Velocity
Cost savings using centralized intelligence and dump devices is secondary.
Primary benefit is better capability and sw feature velocity.
**** HA: Five 9s (99.999%) service availability i.e. <5 mins downtime/yr
**** High feature velocity at all layers: Svc, Mgmt, CP,  and DP.
**** Feature dev to production in weeks rather than quarters.
**** In service upgrade
All tasks/hosts should allow in-service upgrade and restart.
*** Intent Driven Manageability i.e. Compiler: 
+ Reasons 
 ++ Properly configured network is a prerequisite to higher-level network
 functions i.e. Connect network islands using user-role-app-aware policy 
 based routing while applying appropriate security services.
 ++ Eliminate the riskiest interaction agent i.e human.
 ++ Easier and abstract expression avoids low level mgmt ops from human,
 the riskiest interacton agent cause of unavailability. 
 ++ Agility enables evolution e.g. add device, upgrade capacity, etc. 
 to support changing and new app needs.
 ++ Efficiency: SP (Cloud) platform spends most time on mgmt plane.
 ++ Committing intent triggers management system to generate,  version, 
 and statically verify config before pushing to sw components/devices.
 ++ Canary verification with select components.
*** Learn from deployment Experience. Iterate on implementation.
** Design Consequence
*** Avoid new IETF (standardization efforts) or DP switch silicon.
*** Scale up means any upgrade/failure affects substantial % of traffic.
*** TCO per bit 4-10x relative to custom silicon.
*** Akin to B2 connecting DCs => peering edges => End Users.
*** Avoid loops/inefficiency: app aware routing respect BGP policy.
*** WATCH capability on a flow accross all intermediate hops
*** MEASURE use of features to deprecate unused features
*** RELEASE checks, new feature, qualification, and rollout targets
*** CANARY release: fewer Pops, first to lower priority traffic, etc.
*** OPEN
Federated SD-WAN BGP on Controller to avail server CPU cores/memory
vGW Transit supports Internet Table Sizes: IPv4/IPv6: 2017 = 700K/40K; 
2022: 1.4M/140K; FIB: 2019: 1M


ESPRESSO
* Summary
** Application aware routing at Internet-peering scale.
** Host based routing/packet processing
** Fine-grained traffic engineering.

* Peering Edge
** Typically located at Metros
** Objectives
*** Peering with partner AS 
*** Server pool of reverse proxies for TCP termination
*** Caching and content distribution of static content

* Design Objectives
** Support interactive low latency to a global user population
** Line of defense against DoS and related attacks
** Reduce buildout of the backbone network back to DC

* Edge Routers
** Management & Configuration Complexity
*** Few boxes with highest scale BW, ports, and packet processing rates.
** Large DB: Internet-scale forwarding tables: 1M entries
** Large ACLs: complex and large scale firewall and anti-DOS rules
** High end compute for BGP to manage 100s of peering sessions

* Mechanisms
** Externalize most network control from devices leaving mostly
    MPLS DP and latency sensitive control loop functionality.
** PP, routing on Int scale forwarding tables and ACLs to sw PP 
    running on cluster server in edge Pops.
** Centrally enable gobal & app-aware TE system for 
   fine-grained BW mgmt with an e2e per flow performance i.e.
   much better than vanilla distributed BGP-based routing.
** Move BGP to a custom stack running on servers.
* Design Principlies
** Hierarchical Control Plane
Local controllers apply programming rules and app specific traffic
routing policies computed by global controller. 
+ Advantages:
++ Efficiency: Global optimization
++ Reliability: Local operates independently of global
++ Reaction: Local events are treated via local repair while 
waiting for optimal global optimization.
** Fail Static
Data plane maintains last known good state when control plane 
is unavailable for short periods of time. Externalizing BGP stack
from devices decouples data plane and control failure. Allows
upgrade of CP on a live system without impacting DP.
** Software Programmability
Simple HW primitives e.g. MPLS push/pop and forward to NH.
** Testability
Separation of concerns and sw defined and component based 
features allows easier testing.
** Intent Driven Manageability
Controlled and automated configuration updates supports 
large scale, safe, automated, and incremental updates. 
Sub-linear scaling of human overhead in runnning operations and 
reduction of operational errors i.e. main source of errors.
https://research.fb.com/wp-content/uploads/2016/11/robotron_top-down_network_management_at_facebook_scale.pdf?
Built a fully automated configuration pipeline, where a change in intent 
is automatically and safely staged to all devices.
* Design Overview
** Components
TE Controller (Global), Location Controller (Edge Metro), 
Peering Fabric Controller, BGP Speaker, Legacy 
Peering Edge Router, New Peering Fabric (GRE/MLPS 
Packet Processor), and Host with Packet Processor
** Application-Aware Routing
Hosts have Internet-Scale FIB. 
Inbound packets directed from PF to host using IP-GRE, 
where ACLs are applied at hosts.
Outbound packets encode the PF using GRE and MPLS 
the exact output port of PF. PF does not host any large 
scale FIB or ACL for ingress/egress processing.
** BGP peering
Peer ==> PF => IPGRE ==> Server-BGP
* App-aware TE System
** Global Controller
Greedy algorithm assigns traffic to candidate egress device/port. 
*** Output of GC: 
For each <PoP, client-prefix, service-class> = egress-map, where
egress-map = prioritized list of <PR/F, egress-port>, where
service-class encodes traffic priorityservice-class encodes traffic priority
*** Inputs to GC:
**** Peering Routes: 
GC maps user traffic to egress router/port where peering accepts the route.
Feed Aggregator collects routes and BGP attributes from all peers.
**** User BW & Performance:
Volume Aggregator aggregates BW, Goodput, RTT, ReTx, and Queries/Sec reports.
Prefixes are disaggregated until latency characteristics are similar.
**** Link Utilization:
Per Queue link utilization, drops, and link speed from peering devices.
Limits Computer aggregates this information with Volume Aggregator
based user demand to allocate BW per link per service class.
*** Availability
GC availability is guaranteed as multiple instances run accross DCs 
with a leader election using distributed lock.
*** Safety
**** Sanity 
Observed by checks i.e. verifying sanity of all incoming data 
sources and ignoring sources that fail checks.
**** Canary
LCs canary the new map to a small fraction of PPs. End status is reported
to GC. Canary failure causes alarm. Canary success effects new map.
**** Fail-static:
Ignore new maps when validation check fails or when map is significantly 
different than the previous map. 
**** Archive
GC inputs/outputs are archived to revert programming to last known-good state.

Validation mechanisms reduces scope and frequency of production 
outages. Reverting to known good state reduces duration of outages.

** Location Controller
GC changes are dampened due to global impact. Critical changes, such 
as interface failure or sudden decrease in serving capacity is handled locally.
LC acts as a fallback in case of GC failure.
*** Scaling host programming
Multiple eventually consistent LC instances programs edge metro PPs 
identically. Support of more PPs realized by adding more LC instances.
*** Canary
Subset of PPs programmed including control ACL operations. After verifying 
correct operation in canary set wider distribution is realized.
*** Local Routing Fallback
BGP best path destination prefix routing is the default and fail-static route.
Furthermore, subset of routes are mapped to this path to maintain confidence.
*** Fast Recovery from Failures
LC uses internal priority queues to prioritize bad news and local reaction, such
as interface down or route withdrawals over GC programming. 

** Peering Fabric
*** Raven BGP Speaker
Custom BGP stack chosen (i.e. Raven) that is multi-threaded and uses the 
large number of CPU cores and memory on commodity servers. Quagga is 
C-based single threaded with limited testing support.
BGP Peer <==> Raven-Instance and PF <==> {Raven-Instance} i.e. failure of 
RavenTask impacts few BGP peers.
Enable BGP Graceful Restart to allow data traffic flow during Raven restart.
Externalizing network functionality to multiple server racks increases
scale and robustness against power failure.
*** Commodity MPLS Switch
Rather than Internet sized FIB. Program MPLS Forwarding and
IP-GRE encap/decap.
*** Peering Fabric Controller
Installs flow rules to IP-GRE encap BGP packets to BGP server.
Installs flow rules on PF to decap egress packet and forward to NextHop 
based on MPLS label. 
Master-Standby to increase availability.
*** Internet ACL
Partition Internet facing ACLs between those being hit with 
highest volume (5% of ACLs covering 99% of traffic volume)
implemented in HW-PF vs fine-grained filtering implemented 
in Server. 
Server implemented filtering performs more advanced filtering 
(e.g. Google DoS) than available in any router. Design allows
easy insertion of new rules on demand.

** Configuration and Management
Check Human-readable Intent
Compile to lower-level config data consumed by systems.
Changing config-schema requires changing the consumer
not the config management system.
*** Declarative
Simplifies higher level config and workflow automation.
*** Canary
Indirection allows sending config incrementally to systems,
monitor impact, and then roll out for all devices.
*** Depth in Defense
CP is composed of discrete components performing
specific funtion with each validating config snippets 
independently.
*** Fail-Static
If LC connection to config system is lost then LC uses
last known configuration while alerts to fix connection
are processed. LC canaries to subset of devices to 
verify correct behavior.
*** Big Red Button
Quickly, Reliably, and safely disable parts of the system 
at various levels. These buttons are tested nightly.
*** Network Telemetry
Data-Plane changes and reaction time stats (e.g. peer link 
failure or route withdrawal) are streamed to PFC e.g. BGP
sessions timed out when peering link fails. 
Every binary in control plane exports information on 
standard HTTP/RPC endpoint, which is collected and
aggregated using a system like Prometheus.
*** Data plane Monitoring
Run e2e probes traversing regular path traffic. Probes 
also used to verify proper installation of ACLs at hosts.
Also probes used to loop back through PF and various
links to ensure proper path installation.

* Feature and Rollout Velocity
Support independent, asynchronous, and accelerated releases via 
loosely coupled software components. 
- Release Checks
 --  Extensive unit tests, pairwise inter-operability tests and e2e 
 full system tests.
 -- Requires full inter-operability testing accross versions.
 -- Fully automated integration testing, canarying, and rollout of 
 software components.
 -- Many tests run in a production-like QA environment with a 
 full suite of failure, performance, and regression tests.
 -- After test suite pass, the system initiates automated 
 incremental global rollout. Target velocity < 14 days,
 -- Qualification < 1-2 days, and Rollout < 4-5 days.
 -- Critical issues requires manual release, testing, and rollout
 in hours.
 -- Deprecate unused features to maintain clean codebase.
* Evaluation
** Comparison of BGP Speakers
Choices: Quagga, Bird, XORP, or extend Raven.
Quagga optimized significantly in B4.
BGP advertizing and withdrawal convergence time for IPv4/6.
Raven has faster convergence, half memory, and lower latency -
doesn't write route to kernel. Quagga is single threaded and 
does not explot multiple cores on machines.
Servers have 10x CPU cores and memory relative to routers.
** Big Red Button
Disable peering (takes 4.12s) or de-configure BGP peers via 
intent change (takes 19.9s).
** Host Processing
Immutable: LPM allows lock-free packet processing. Install
shadow LPM. Update pkt processing paths to new LPM afterwards.
Data Structures: IPv4 multibit trie. IPv6 binary trie.
IPv4/v6 1.9M Prefixes-service class tuples.
RAM: 2017: 1.3GB at 99 percentile. 
CPU Overhead: 37Gbps @ 3Mpps 2.2% of CPU for binary trie LPM.
* Experience
Go into production quickly with limited deployment. Iterate on 
implementation based on experience.


ROBOTRON
* Summary
** Network Management Challenges
*** Distributed Configurations
High level intent (e.g. provisioning decisions) translated into
distributed low level device configs is difficult and error prone
due to heterogeneous config options e.g. migrate a circuit 
between routers: change IP address, BGP sessions, interfaces,
drain/undrain to avoid traffic interruption, etc.
*** Multiple Domains
Networks of Network (i.e. Pops, BB, DCs) each with different 
characteristics. 
*** Versioning
Different part of network exist in different versions of topologies,
devices, link speeds, and configs.
*** Dependency
Adding a new element would require changing config on all dependent
elements e.g. add router to IBGP mesh.
*** Vendor Differences
Often full advantage is realized via vendor specific CLI, configs, or APIs
that vary between vendor HW and OS versions.
*** Configuration-as-code
Minimize human interaction and number of workflows via codifying
logic to ensure dependencies are followed and outcome device
configs are deterministic, reproducible, and consistent.
No manual login to any network device for management tasks.
*** Validation
Sanity checks e.g. IP address of p2p circuit rejected if each belong 
to different subnets.
*** Extensibility
New device models, circuit types, DC/Pop sites, network topologies, etc.
Allow network engineers to extend functionality with templates, tool
configs, and code changes.

** Network and Use Cases
